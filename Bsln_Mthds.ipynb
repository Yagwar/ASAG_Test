{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Short Answer Grading\n",
    "\n",
    "Calificación de una base de preguntas dataset de semeval'13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "#import sswe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import theano\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Text\n",
    "import gensim, logging\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn import cross_validation\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from collections import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "\n",
    "# Clasifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# LSTM \n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# xml reader\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_fname = \"/home/aecarrillor/ASAG/GoogleNews-vectors-negative300.bin\"\n",
    "w2vModel = Doc2Vec.load_word2vec_format(_fname, binary=True)\n",
    "stp_wrds=stopwords.words('english')## Caching the stopwords object. calling in tokenizer function makes a bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_data(text,remove_stpwrds=False):\n",
    "    nst=tknz_sntnc(text,remove_stpwrds)\n",
    "    text_len=len(nst)\n",
    "    if len(nst)==0:\n",
    "        text_len=0.0001\n",
    "    wrd_in_w2vec=sum([1  if word in w2vModel else 0.0 for word in nst])\n",
    "    match_prop=wrd_in_w2vec/float(text_len)\n",
    "    return [text_len,wrd_in_w2vec,match_prop]\n",
    "\n",
    "def tknz_sntnc(text, remove_stpwrds):\n",
    "    tokenizer = RegexpTokenizer('\\w+\\'+\\w+|\\w+')#r'\\w+'\n",
    "    tokens=tokenizer.tokenize(text)\n",
    "    tokens=[wrd.lower()for wrd in tokens]\n",
    "    if remove_stpwrds:#==True:\n",
    "        filtered_words = [word for word in tokens if word not in stp_wrds]#stp_wrds must be declared as a global object\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return tokens\n",
    "    # Meter stopwords removal\n",
    "    \n",
    "def tkns2vec(tokens, normalize):\n",
    "    if len(tokens)==0:\n",
    "        return np.array([0 for i in xrange(300)])# Si el tokenizador da un vector vacío, se corrige on un vector de ceros\n",
    "    ar_w2v=[w2vModel[word] if word in w2vModel else np.array([0 for i in xrange(300)]) for word in tokens]# si la palabra no está en el embedding, arroja un vector de ceros.\n",
    "    #ar_w2v=[w2vModel[word] if word in w2vModel else np.array([\"\"\"\"\"\"\"\" for i in xrange(300)]) for word in tokens]# si la palabra no está en el embedding, arroja un vector de numeros aleatorios con seed en la palabra ??? cómo obtener el seed.\n",
    "    if normalize==True:\n",
    "        return sum(ar_w2v)/len(ar_w2v)\n",
    "    else:\n",
    "        return sum(ar_w2v)\n",
    "    #OOV_words=[\"\" if word in w2vModel else word for word in tokens]\n",
    "\n",
    "def w2vec_txt(text, normalize, remove_stpwrds):\n",
    "    tokens=tknz_sntnc(text,remove_stpwrds)\n",
    "    return tkns2vec(tokens, normalize)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xml_2_900_vec(path_files, normalize=False, remove_stpwrds=False):\n",
    "\n",
    "    #Variables internas\n",
    "    all_files =  glob.glob(path_files+'*.xml')\n",
    "\n",
    "    #Procesos\n",
    "\n",
    "    ## crear array con id de pregunta, textos combinados y calificación.\n",
    "    id_question=[]# Identificador de la pregunta (etiqueta de clase).\n",
    "    calif=[]# Marca de criterio.\n",
    "    quest=[]#Pregunta.\n",
    "    ref_answer=[]#Respuesta de referencia.\n",
    "    stud_answ=[]#Respuesta de estudiante.\n",
    "\n",
    "\n",
    "    for doc in all_files:\n",
    "        tree = ET.parse(doc)\n",
    "        root = tree.getroot()\n",
    "        question=root[0].text\n",
    "        grade_st=[branch.attrib[\"accuracy\"] for branch in root[2]]\n",
    "        answers_st=[branch.text for branch in root[2]]\n",
    "        answers_ref=[branch.text for branch in root[1]]# If there are more of 1 reference answer...\n",
    "        #print (doc, root.attrib[\"id\"])\n",
    "        for ith_ans,st_answ in enumerate(answers_st):\n",
    "            quest.append(question)\n",
    "            ref_answer.append(answers_ref[0])\n",
    "            stud_answ.append(st_answ)\n",
    "            id_question.append(root.attrib[\"id\"])\n",
    "            st_ans_calif=str(grade_st[ith_ans])\n",
    "    #             calif.extend([st_ans_calif]) # Categorías en texto para clasificadores multiclase\n",
    "            calif.extend([1 if st_ans_calif==\"correct\" else 0])\n",
    "            #print(ith_ans)\n",
    "    arr_in=np.column_stack((id_question,quest,ref_answer,stud_answ))\n",
    "    arr_w2v=[[w2vec_txt(text,normalize, remove_stpwrds) for text in ans[1:]]for ans in arr_in]\n",
    "    vec_fin=[list(arr2v[0])+list(arr2v[1])+list(arr2v[2])for arr2v in arr_w2v]\n",
    "    return [vec_fin,calif,arr_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-way \n",
    "\n",
    "###ScientisBank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 4969\n",
      "UA: 540\n",
      "UD: 4562\n",
      "UQ: 733\n"
     ]
    }
   ],
   "source": [
    "nrmalz=True\n",
    "rmv_stp=False\n",
    "\n",
    "stud_answ=xml_2_900_vec('S13T7Data/2and3way/training/2way/sciEntsBank/',nrmalz,rmv_stp)\n",
    "X_train_w2v=stud_answ[0]\n",
    "y_train_w2v=stud_answ[1]\n",
    "print (\"Training:\",len(y_train_w2v))\n",
    "\n",
    "unseen_ans=xml_2_900_vec('S13T7Data/2and3way/test/2way/sciEntsBank/test-unseen-answers/',nrmalz,rmv_stp)\n",
    "X_unseen_ans_w2v=unseen_ans[0]\n",
    "y_unseen_ans_w2v=unseen_ans[1]\n",
    "print (\"UA:\",len(y_unseen_ans_w2v))\n",
    "\n",
    "unseen_doms=xml_2_900_vec('S13T7Data/2and3way/test/2way/sciEntsBank/test-unseen-domains/',nrmalz,rmv_stp)\n",
    "X_unseen_doms_w2v=unseen_doms[0]\n",
    "y_unseen_doms_w2v=unseen_doms[1]\n",
    "print (\"UD:\",len(y_unseen_doms_w2v))\n",
    "\n",
    "unseen_qstns=xml_2_900_vec('S13T7Data/2and3way/test/2way/sciEntsBank/test-unseen-questions/',nrmalz,rmv_stp)\n",
    "X_unseen_qstns_w2v=unseen_qstns[0]\n",
    "y_unseen_qstns_w2v=unseen_qstns[1]\n",
    "print (\"UQ:\",len(y_unseen_qstns_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97943544733564303"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.array(stud_answ[2])[:,1:]\n",
    "OOV_stud_ans_stop_prop=np.array([[txt_data(txt,True) for txt in answrs]for answrs in data])[:,:,2]\n",
    "props_OOV_arr=np.array(OOV_stud_ans_stop_prop)\n",
    "props_OOV_arr\n",
    "np.mean(props_OOV_arr)\n",
    "\n",
    "#plt.hist(OOV_stud_ans_stop_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a', u'and', u'of', u'to', u'mightn', u'mustn']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stpwinW2vec=[1 if stp in w2vModel else 0 for stp in stp_wrds]\n",
    "np.mean(stpwinW2vec)\n",
    "stpwNotInW2vec=[stp for stp in stp_wrds if stp not in w2vModel ]\n",
    "#word for word in tokens if word not in stp_wrds\n",
    "stpwNotInW2vec\n",
    "#w2vModel[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UA</th>\n",
       "      <th>UQ</th>\n",
       "      <th>UD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.2517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.4989</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>0.4871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM -Lineal</th>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.5094</td>\n",
       "      <td>0.3962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.6699</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.0460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UA      UQ      UD\n",
       "Logistic       0.5509  0.4615  0.2517\n",
       "Naive Bayes    0.4989  0.5928  0.4871\n",
       "SVM -Lineal    0.5347  0.5094  0.3962\n",
       "Random Forest  0.6699  0.2027  0.0460"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create classifiers\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "svc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", n_iter=1500)# probar con otras funciones perdida # Mean Square Error\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f1_UA=[]\n",
    "f1_UD=[]\n",
    "f1_UQ=[]\n",
    "\n",
    "clasifs_w2v=[]\n",
    "\n",
    "for clf, name in [(lr, 'Logistic'),\n",
    "                  (gnb, 'Naive Bayes'),\n",
    "                  (svc, 'SVM -Lineal'),\n",
    "                  (rfc, 'Random Forest')]:\n",
    "    clf.fit(X_train_w2v, y_train_w2v)\n",
    "    yhat_UA = clf.predict(X_unseen_ans_w2v)\n",
    "    yhat_UD = clf.predict(X_unseen_doms_w2v)\n",
    "    yhat_UQ = clf.predict(X_unseen_qstns_w2v)\n",
    "    f1_UA.append(f1(y_unseen_ans_w2v, yhat_UA, average='weighted'))####### F1 weighted Average parameter\n",
    "    f1_UD.append(f1(y_unseen_doms_w2v, yhat_UD, average='weighted'))####### F1 weighted Average parameter\n",
    "    f1_UQ.append(f1(y_unseen_qstns_w2v, yhat_UQ, average='weighted'))####### F1 weighted Average parameter\n",
    "    clasifs_w2v.append(name)\n",
    "\n",
    "results_w2v=pd.DataFrame(np.column_stack(([round(scr,4) for scr in f1_UA],[round(scr,4) for scr in f1_UQ],[round(scr,4) for scr in f1_UD])),\n",
    "                         columns=[\"UA\",\"UQ\", \"UD\"], index=clasifs_w2v)\n",
    "results_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "- se hace fuera del loop de los clasifacdores para poder explorar parámetros.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name MLPClassifier",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-248-6113b65dfa44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name MLPClassifier"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-247-87451c6cc60d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m clf = MLPClassifier(solver='lbfgs', alpha=0.1,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     hidden_layer_sizes=(300,60,1), random_state=1)\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_w2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0myhat_UA_MLP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_unseen_ans_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLPClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=0.1,\n",
    "                    hidden_layer_sizes=(300,60,1), random_state=1)\n",
    "clf.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "yhat_UA_MLP = clf.predict(X_unseen_ans_w2v)\n",
    "yhat_UD_MLP = clf.predict(X_unseen_doms_w2v)\n",
    "yhat_UQ_MLP = clf.predict(X_unseen_qstns_w2v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bettle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stud_answ=xml_2_900_vec('S13T7Data/2and3way/training/2way/beetle/',nrmalz,rmv_stp)\n",
    "X_train_w2v=stud_answ[0]\n",
    "y_train_w2v=stud_answ[1]\n",
    "\n",
    "unseen_ans=xml_2_900_vec('S13T7Data/2and3way/test/2way/beetle/test-unseen-answers/',nrmalz,rmv_stp)\n",
    "X_unseen_ans_w2v=unseen_ans[0]\n",
    "y_unseen_ans_w2v=unseen_ans[1]\n",
    "\n",
    "#This dataset doesn't have Unknown Domain\n",
    "# unseen_doms=xml_2_900_vec('S13T7Data/2and3way/test/2way/beetle/test-unseen-domains/',nrmalz,rmv_stp)\n",
    "# X_unseen_doms_w2v=unseen_doms[0]\n",
    "# y_unseen_doms_w2v=unseen_doms[1]\n",
    "\n",
    "unseen_qstns=xml_2_900_vec('S13T7Data/2and3way/test/2way/beetle/test-unseen-questions/',nrmalz,rmv_stp)\n",
    "X_unseen_qstns_w2v=unseen_qstns[0]\n",
    "y_unseen_qstns_w2v=unseen_qstns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UA</th>\n",
       "      <th>UQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.6023</td>\n",
       "      <td>0.5258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.4804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM -Lineal</th>\n",
       "      <td>0.6018</td>\n",
       "      <td>0.5260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.7688</td>\n",
       "      <td>0.5374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UA      UQ\n",
       "Logistic       0.6023  0.5258\n",
       "Naive Bayes    0.5836  0.4804\n",
       "SVM -Lineal    0.6018  0.5260\n",
       "Random Forest  0.7688  0.5374"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create classifiers\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "svc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", n_iter=1500)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f1_UA=[]\n",
    "#f1_UD=[]\n",
    "f1_UQ=[]\n",
    "\n",
    "clasifs_w2v=[]\n",
    "\n",
    "for clf, name in [(lr, 'Logistic'),\n",
    "                  (gnb, 'Naive Bayes'),\n",
    "                  (svc, 'SVM -Lineal'),\n",
    "                  (rfc, 'Random Forest')]:\n",
    "    clf.fit(X_train_w2v, y_train_w2v)\n",
    "    yhat_UA = clf.predict(X_unseen_ans_w2v)\n",
    "    #yhat_UD = clf.predict(X_unseen_doms_w2v)\n",
    "    yhat_UQ = clf.predict(X_unseen_qstns_w2v)\n",
    "    f1_UA.append(f1(y_unseen_ans_w2v, yhat_UA, average='weighted'))####### F1 weighted Average parameter\n",
    "    #f1_UD.append(f1(y_unseen_doms_w2v, yhat_UD, average='weighted'))####### F1 weighted Average parameter\n",
    "    f1_UQ.append(f1(y_unseen_qstns_w2v, yhat_UQ, average='weighted'))####### F1 weighted Average parameter\n",
    "    clasifs_w2v.append(name)\n",
    "\n",
    "results_w2v=pd.DataFrame(np.column_stack(([round(scr,4) for scr in f1_UA],[round(scr,4) for scr in f1_UQ])),\n",
    "                         columns=[\"UA\", \"UQ\"], index=clasifs_w2v)\n",
    "results_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
